1
00:00:00,231 --> 00:00:03,700
[MUSIC]

2
00:00:07,360 --> 00:00:10,896
Hi, in this lecture we're
going to introduce some

3
00:00:10,896 --> 00:00:13,580
efficient pattern mining methods.

4
00:00:15,560 --> 00:00:20,690
Since we already know frequent pattern
is a very important concept and

5
00:00:20,690 --> 00:00:24,840
mining frequent pattern in large
datasets could be challenging.

6
00:00:24,840 --> 00:00:28,980
So then, we are going to study
some interesting principles and

7
00:00:28,980 --> 00:00:31,589
algorithms for mining frequent patterns.

8
00:00:32,780 --> 00:00:38,160
But first, we are going to introduce you
a very property of frequent patterns,

9
00:00:38,160 --> 00:00:42,340
which is called the downward closure
property of frequent patterns.

10
00:00:43,490 --> 00:00:46,500
Let's look at this simple
transaction database, TDB1.

11
00:00:46,500 --> 00:00:51,434
It contains only two transactions,
T sub 1 and T sub 2.

12
00:00:52,700 --> 00:00:56,350
Suppose we get a frequent
itemset a1 to a50.

13
00:00:58,170 --> 00:01:04,720
Then we actually can clearly
see all its subsets like a1,

14
00:01:04,720 --> 00:01:08,680
a2 or
a1 a2 is itemset that are all frequent.

15
00:01:09,980 --> 00:01:12,840
Then you may wonder,
there may, must be some

16
00:01:12,840 --> 00:01:18,190
interesting hidden relationships
among different frequent itemsets.

17
00:01:19,470 --> 00:01:24,350
Actually, there is one called downward
closure property of frequent patterns,

18
00:01:24,350 --> 00:01:27,370
which is also called the Apriori property,
okay?

19
00:01:29,460 --> 00:01:31,780
Now, let's look at this.

20
00:01:31,780 --> 00:01:37,030
Suppose we know {beer, diaper,
nuts}, this itemset is frequent.

21
00:01:37,030 --> 00:01:43,023
Obviously {beer, diaper} should be
frequent as well, because any transaction

22
00:01:43,023 --> 00:01:50,360
which contains {beer, diaper, nuts} must
also contain {beer, diaper} as an itemset.

23
00:01:50,360 --> 00:01:52,201
That's why the {beer,

24
00:01:52,201 --> 00:01:58,960
diaper} as a itemset should be at least
as frequent as {beer, diaper, nuts}.

25
00:01:58,960 --> 00:02:01,600
So we can easily derive this property.

26
00:02:01,600 --> 00:02:05,020
Said any subsets of
a frequent itemset must be

27
00:02:05,020 --> 00:02:09,720
frequent if we keep the minimal
support ratio as the same.

28
00:02:11,090 --> 00:02:17,160
So in that context, we can derive
an efficient mining methodology.

29
00:02:17,160 --> 00:02:19,150
The general philosophy is this.

30
00:02:19,150 --> 00:02:26,290
If you find an itemset S,
any of its subset is infrequent,

31
00:02:26,290 --> 00:02:31,870
then there's no chance for S to become
frequent because based on this Apriori

32
00:02:31,870 --> 00:02:37,129
property, then we do not even
have to consider to mine S.

33
00:02:38,680 --> 00:02:42,460
This actually turns out to be
a sharp knife for pruning.

34
00:02:43,930 --> 00:02:48,650
So, this Apriori pruning and,
based on this,

35
00:02:48,650 --> 00:02:53,990
it generates quite a lot of
scalable pattern mining methods.

36
00:02:55,290 --> 00:03:01,420
So the first Apriori pruning principle
was discovered by Rakesh Agrawal and

37
00:03:01,420 --> 00:03:05,560
Srikant in VLDB 1994.

38
00:03:05,560 --> 00:03:11,930
Heikki Mannila in KDD 94 workshop
also generated a similar methodology.

39
00:03:12,930 --> 00:03:16,622
The methodology generally
says if there's any subset,

40
00:03:16,622 --> 00:03:19,308
any itemset which is infrequent,

41
00:03:19,308 --> 00:03:25,310
then it's superset should not even be
considered or not even be generated.

42
00:03:26,670 --> 00:03:27,830
Based on this,

43
00:03:27,830 --> 00:03:33,770
there are three major approaches
developed in subsequent studies.

44
00:03:33,770 --> 00:03:37,440
One essentially is Apriori.

45
00:03:37,440 --> 00:03:42,470
The first representative
work was published in

46
00:03:42,470 --> 00:03:46,780
VLDB 1994 called level-wise,
join-based approach.

47
00:03:46,780 --> 00:03:50,370
Another method was
developed by Zaki et al.

48
00:03:50,370 --> 00:03:57,980
and what they got, called Eclat,
is based on vertical data format.

49
00:03:57,980 --> 00:04:00,970
Then the third approach
essentially is pattern-based.

50
00:04:00,970 --> 00:04:04,890
It's frequent pattern projection and
growth.

51
00:04:04,890 --> 00:04:13,330
Pattern growth approach or FPgrowth,
developed by us in year 2000.

52
00:04:13,330 --> 00:04:23,330
[MUSIC]

