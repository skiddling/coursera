1
00:00:00,000 --> 00:00:07,929
[MUSIC]

2
00:00:07,929 --> 00:00:13,041
Since the proposal of the famous
Apriori algorithm there have been

3
00:00:13,041 --> 00:00:18,870
many interesting extensions or
improvements to this method.

4
00:00:18,870 --> 00:00:20,670
Let's just examine some of them, 'kay.

5
00:00:22,570 --> 00:00:26,680
So the first line of work is
how to reduce the number of

6
00:00:26,680 --> 00:00:30,020
passes of transaction database scans.

7
00:00:30,020 --> 00:00:35,930
We know that database scan involve disc
accessing, which could be quite expensive,

8
00:00:35,930 --> 00:00:40,070
but if you want to find
size ten frequent itemsets,

9
00:00:40,070 --> 00:00:42,999
likely you have to scan
this database ten times.

10
00:00:44,020 --> 00:00:49,920
That's quite expensive to
reduce a number of scans, so

11
00:00:49,920 --> 00:00:51,960
there are several publications.

12
00:00:51,960 --> 00:00:54,990
One is called partitioning method,

13
00:00:54,990 --> 00:01:00,110
we're going to introduce, another one
called dynamic itemset counting method.

14
00:01:00,110 --> 00:01:03,790
You probably notice the first
author search brand

15
00:01:03,790 --> 00:01:07,510
who was a PG skilled in
of Stanford University.

16
00:01:07,510 --> 00:01:11,560
The second year, 1998,
he occurred to propose one of

17
00:01:11,560 --> 00:01:15,600
the most famous algorithm page rank and
set up a Google company.

18
00:01:17,440 --> 00:01:23,960
So the, another line of the work is how
to shrink the number of candidates.

19
00:01:23,960 --> 00:01:29,110
You generated very large number
of candidate sets in many cases.

20
00:01:29,110 --> 00:01:34,630
How to shrink the number of candidate sets
to be generated could be a big saving.

21
00:01:34,630 --> 00:01:39,020
Hashing method is interesting one for
mining max patterns.

22
00:01:39,020 --> 00:01:43,810
Bayardo pro, also proposed pruning
by support lower bounding.

23
00:01:43,810 --> 00:01:48,580
Another interesting method is Toivonen,
proposed sampling method.

24
00:01:49,740 --> 00:01:53,990
The Surlines work is to explore
some special data structures,

25
00:01:53,990 --> 00:01:59,440
like some tree projection method
using H3 to do H-miner, and,

26
00:01:59,440 --> 00:02:02,625
hypercube decomposition is
another interesting method.

27
00:02:02,625 --> 00:02:07,760
We're going to introduce a new
tree structure called FP tree.

28
00:02:07,760 --> 00:02:10,832
Let's look the first one,
partitioning method,

29
00:02:10,832 --> 00:02:16,420
partitioning method guarantee you only
need to scan data twice, you know,

30
00:02:16,420 --> 00:02:20,120
before you generate all the frequent
itemsets, no matter for

31
00:02:20,120 --> 00:02:26,710
how many case it is, so they observed
a very interesting property core.

32
00:02:26,710 --> 00:02:32,080
Any itemset potentially frequent in the
transaction database must be frequent in

33
00:02:32,080 --> 00:02:38,240
at least on of its partitions,
that means suppose you get a global tr,

34
00:02:38,240 --> 00:02:43,785
big database, big transaction database,
you partition them into k partitions.

35
00:02:43,785 --> 00:02:49,400
If an itemset X is not
frequenting any one of

36
00:02:49,400 --> 00:02:55,190
these k partitions, it cannot be frequent
in this global transaction database.

37
00:02:55,190 --> 00:02:55,980
Why?

38
00:02:55,980 --> 00:02:56,574
The frequency.

39
00:02:56,574 --> 00:03:00,680
If you got an itemset X,

40
00:03:00,680 --> 00:03:06,140
which is not frequent in the first one,
not frequent in the second one,

41
00:03:06,140 --> 00:03:10,795
not even, not frequent in the case one,
so you add them together.

42
00:03:10,795 --> 00:03:14,005
You add all these support together
you get a global support.

43
00:03:14,005 --> 00:03:18,265
You add all these database together
you get a global database.

44
00:03:18,265 --> 00:03:23,955
You pretty can see if everyone is
smaller than sigma times its size,

45
00:03:23,955 --> 00:03:25,055
then, the finally,

46
00:03:25,055 --> 00:03:31,670
the global one will also smaller than six
sigma times the size of global database.

47
00:03:31,670 --> 00:03:37,065
That's why at least one of these
database contain must be frequent.

48
00:03:37,065 --> 00:03:42,350
'Kay, so with this observation,
'kay you can

49
00:03:42,350 --> 00:03:46,840
see this partition method
we need only two scans.

50
00:03:46,840 --> 00:03:52,320
The first scan is you partition
database into k partition database.

51
00:03:52,320 --> 00:03:53,880
How do you partition them?

52
00:03:53,880 --> 00:03:59,060
Each partition you want them
to fit into the main memory.

53
00:03:59,060 --> 00:03:59,680
Why?

54
00:03:59,680 --> 00:04:04,970
Because no matter how you scan them,
you are not involved in any database iOS.

55
00:04:04,970 --> 00:04:11,220
So that's the reason you can scan once,
you put them in the database, you can work

56
00:04:11,220 --> 00:04:17,190
on it, derive k frequent items that, for
no matter how many case you can get, okay.

57
00:04:17,190 --> 00:04:22,670
That means you can derive all
the local frequent itemsets, okay.

58
00:04:22,670 --> 00:04:29,810
Then based on this property, you can
do the second scan like this, okay.

59
00:04:29,810 --> 00:04:35,520
You can say, since any frequent
itemset in one of these patron

60
00:04:35,520 --> 00:04:40,830
actions can be potentially frequent so
the globally candidate sets, is,

61
00:04:40,830 --> 00:04:48,130
which is frequenting any one of them,
it will be a global candidate itemset.

62
00:04:48,130 --> 00:04:52,245
Then the second scan
just can't against each

63
00:04:52,245 --> 00:04:58,090
database combos counts of
the global candidate set, okay.

64
00:04:58,090 --> 00:04:59,604
Then you will get their count,

65
00:04:59,604 --> 00:05:05,400
you add them together, you will be able to
either derive global frequent patterns,

66
00:05:05,400 --> 00:05:10,960
that's why this method guaranteed
to scan database only twice.

67
00:05:10,960 --> 00:05:12,539
So this is pretty interesting.

68
00:05:14,460 --> 00:05:17,614
Another method or
I'm going to introduce you called di,

69
00:05:17,614 --> 00:05:19,606
directly hashing and pruning.

70
00:05:19,606 --> 00:05:23,391
This method is trying
to reduce a number of

71
00:05:23,391 --> 00:05:28,705
candidate sets based on hashing and
pruning techniques.

72
00:05:28,705 --> 00:05:32,351
The general philosophy can be like this,
okay.

73
00:05:32,351 --> 00:05:39,266
The observation is if the k-itemset is
frequent, then in this hashing bucket,

74
00:05:39,266 --> 00:05:45,340
it contains several potential itemsets and
must be frequent.

75
00:05:45,340 --> 00:05:48,610
So if the hash count is not frequent,

76
00:05:48,610 --> 00:05:53,960
then any one of them cannot be frequent,
the general philosophy like this, okay.

77
00:05:53,960 --> 00:05:57,930
Why you are going to derive
the frequent one-item set?

78
00:05:57,930 --> 00:06:03,549
In the meantime, you set of a hash entry
for the potential two-item sets, but

79
00:06:03,549 --> 00:06:09,171
why you said hash entry rather than count
each one because there could be many,

80
00:06:09,171 --> 00:06:10,810
many distinct items.

81
00:06:10,810 --> 00:06:16,193
So the frequent two itemsets before you
get the frequent one, the candidate two

82
00:06:16,193 --> 00:06:21,492
could be really, really big, but if you
set a hansh, hash entry to get several

83
00:06:21,492 --> 00:06:26,587
items, you share one hash entry,
the whole hash table will not be too big.

84
00:06:26,587 --> 00:06:31,352
Then the interesting thing is why
do you derive frequent one itemset?

85
00:06:31,352 --> 00:06:37,940
You also can cut many two itemsets,
they can not be frequent.

86
00:06:37,940 --> 00:06:43,580
For example suppose our minimum support
can be 70, then this one has only 35,

87
00:06:43,580 --> 00:06:49,180
this one has only 58,
that implies those items,

88
00:06:49,180 --> 00:06:55,270
none of those items can be frequent,
because they share this entry.

89
00:06:55,270 --> 00:06:58,180
Their support come this here,
is still below the threshold.

90
00:06:58,180 --> 00:07:03,718
So, only the second one, this bd,
be, de, the support is quite big.

91
00:07:03,718 --> 00:07:08,135
Pass the support threshold these could be

92
00:07:08,135 --> 00:07:12,396
This may contain the real candidate sets,
okay.

93
00:07:12,396 --> 00:07:15,186
That's a reading you can
reduce the number of

94
00:07:15,186 --> 00:07:19,385
candidates substantially using
this hashing and pruning method.

95
00:07:19,385 --> 00:07:29,385
[MUSIC]

